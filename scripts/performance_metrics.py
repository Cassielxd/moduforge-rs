#!/usr/bin/env python3
"""
ModuForge-RS æ€§èƒ½æŒ‡æ ‡æ”¶é›†å’Œåˆ†æç³»ç»Ÿ

åŠŸèƒ½ï¼š
- æ”¶é›†åŸºå‡†æµ‹è¯•ç»“æœ
- å­˜å‚¨å†å²æ•°æ®
- æ€§èƒ½å›å½’æ£€æµ‹
- ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
"""

import sqlite3
import json
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import argparse
import logging
from dataclasses import dataclass
from scipy import stats

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    crate_name: str
    benchmark_name: str
    duration_ns: int
    memory_usage_bytes: int
    cpu_utilization_percent: float
    timestamp: str
    git_commit: Optional[str] = None
    metadata: Optional[Dict] = None

class PerformanceDatabase:
    """æ€§èƒ½æ•°æ®åº“ç®¡ç†å™¨"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„"""
        conn = sqlite3.connect(self.db_path)
        
        # åŸºå‡†æµ‹è¯•ç»“æœè¡¨
        conn.execute('''
            CREATE TABLE IF NOT EXISTS benchmark_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                crate_name TEXT NOT NULL,
                benchmark_name TEXT NOT NULL,
                duration_ns INTEGER NOT NULL,
                memory_usage_bytes INTEGER DEFAULT 0,
                cpu_utilization_percent REAL DEFAULT 0.0,
                timestamp DATETIME NOT NULL,
                git_commit TEXT,
                metadata_json TEXT,
                UNIQUE(crate_name, benchmark_name, timestamp)
            )
        ''')
        
        # æ€§èƒ½åŸºçº¿è¡¨
        conn.execute('''
            CREATE TABLE IF NOT EXISTS performance_baselines (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                crate_name TEXT NOT NULL,
                benchmark_name TEXT NOT NULL,
                baseline_duration_ns INTEGER NOT NULL,
                baseline_timestamp DATETIME NOT NULL,
                git_commit TEXT,
                is_active BOOLEAN DEFAULT 1,
                UNIQUE(crate_name, benchmark_name, is_active)
            )
        ''')
        
        # å›å½’è­¦æŠ¥è¡¨
        conn.execute('''
            CREATE TABLE IF NOT EXISTS regression_alerts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                crate_name TEXT NOT NULL,
                benchmark_name TEXT NOT NULL,
                current_duration_ns INTEGER NOT NULL,
                baseline_duration_ns INTEGER NOT NULL,
                regression_percent REAL NOT NULL,
                severity TEXT NOT NULL,
                timestamp DATETIME NOT NULL,
                git_commit TEXT,
                resolved BOOLEAN DEFAULT 0
            )
        ''')
        
        # åˆ›å»ºç´¢å¼•
        conn.execute('CREATE INDEX IF NOT EXISTS idx_results_crate_time ON benchmark_results(crate_name, timestamp)')
        conn.execute('CREATE INDEX IF NOT EXISTS idx_results_benchmark ON benchmark_results(benchmark_name)')
        conn.execute('CREATE INDEX IF NOT EXISTS idx_alerts_unresolved ON regression_alerts(resolved, timestamp)')
        
        conn.commit()
        conn.close()
        
        logger.info(f"æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {self.db_path}")
    
    def insert_results(self, results: List[BenchmarkResult]):
        """æ’å…¥åŸºå‡†æµ‹è¯•ç»“æœ"""
        conn = sqlite3.connect(self.db_path)
        
        for result in results:
            metadata_json = json.dumps(result.metadata) if result.metadata else None
            
            conn.execute('''
                INSERT OR REPLACE INTO benchmark_results
                (crate_name, benchmark_name, duration_ns, memory_usage_bytes, 
                 cpu_utilization_percent, timestamp, git_commit, metadata_json)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                result.crate_name,
                result.benchmark_name, 
                result.duration_ns,
                result.memory_usage_bytes,
                result.cpu_utilization_percent,
                result.timestamp,
                result.git_commit,
                metadata_json
            ))
        
        conn.commit()
        conn.close()
        
        logger.info(f"æ’å…¥ {len(results)} æ¡åŸºå‡†æµ‹è¯•ç»“æœ")
    
    def get_baseline(self, crate_name: str, benchmark_name: str) -> Optional[Tuple[int, str]]:
        """è·å–æ€§èƒ½åŸºçº¿"""
        conn = sqlite3.connect(self.db_path)
        
        result = conn.execute('''
            SELECT baseline_duration_ns, git_commit
            FROM performance_baselines
            WHERE crate_name = ? AND benchmark_name = ? AND is_active = 1
        ''', (crate_name, benchmark_name)).fetchone()
        
        conn.close()
        
        return result if result else None
    
    def set_baseline(self, crate_name: str, benchmark_name: str, duration_ns: int, git_commit: str):
        """è®¾ç½®æ€§èƒ½åŸºçº¿"""
        conn = sqlite3.connect(self.db_path)
        
        # ç¦ç”¨ç°æœ‰åŸºçº¿
        conn.execute('''
            UPDATE performance_baselines 
            SET is_active = 0 
            WHERE crate_name = ? AND benchmark_name = ?
        ''', (crate_name, benchmark_name))
        
        # æ’å…¥æ–°åŸºçº¿
        conn.execute('''
            INSERT INTO performance_baselines
            (crate_name, benchmark_name, baseline_duration_ns, baseline_timestamp, git_commit)
            VALUES (?, ?, ?, ?, ?)
        ''', (crate_name, benchmark_name, duration_ns, datetime.now().isoformat(), git_commit))
        
        conn.commit()
        conn.close()
        
        logger.info(f"è®¾ç½®åŸºçº¿: {crate_name}/{benchmark_name} = {duration_ns}ns")

class RegressionDetector:
    """æ€§èƒ½å›å½’æ£€æµ‹å™¨"""
    
    def __init__(self, db: PerformanceDatabase, threshold_percent: float = 10.0):
        self.db = db
        self.threshold_percent = threshold_percent
    
    def detect_regressions(self, results: List[BenchmarkResult]) -> List[Dict]:
        """æ£€æµ‹æ€§èƒ½å›å½’"""
        alerts = []
        
        for result in results:
            baseline = self.db.get_baseline(result.crate_name, result.benchmark_name)
            
            if baseline:
                baseline_duration, baseline_commit = baseline
                current_duration = result.duration_ns
                
                # è®¡ç®—å˜åŒ–ç™¾åˆ†æ¯”
                change_percent = ((current_duration - baseline_duration) / baseline_duration) * 100
                
                if change_percent > self.threshold_percent:
                    severity = self._get_severity(change_percent)
                    
                    alert = {
                        'crate_name': result.crate_name,
                        'benchmark_name': result.benchmark_name,
                        'current_duration_ns': current_duration,
                        'baseline_duration_ns': baseline_duration,
                        'regression_percent': change_percent,
                        'severity': severity,
                        'timestamp': result.timestamp,
                        'git_commit': result.git_commit,
                        'baseline_commit': baseline_commit
                    }
                    
                    alerts.append(alert)
                    self._save_alert(alert)
        
        return alerts
    
    def _get_severity(self, change_percent: float) -> str:
        """æ ¹æ®å˜åŒ–ç™¾åˆ†æ¯”ç¡®å®šä¸¥é‡æ€§"""
        if change_percent >= 50:
            return "CRITICAL"
        elif change_percent >= 25:
            return "HIGH"
        elif change_percent >= 15:
            return "MEDIUM"
        else:
            return "LOW"
    
    def _save_alert(self, alert: Dict):
        """ä¿å­˜å›å½’è­¦æŠ¥"""
        conn = sqlite3.connect(self.db.db_path)
        
        conn.execute('''
            INSERT INTO regression_alerts
            (crate_name, benchmark_name, current_duration_ns, baseline_duration_ns,
             regression_percent, severity, timestamp, git_commit)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            alert['crate_name'],
            alert['benchmark_name'],
            alert['current_duration_ns'],
            alert['baseline_duration_ns'],
            alert['regression_percent'],
            alert['severity'],
            alert['timestamp'],
            alert['git_commit']
        ))
        
        conn.commit()
        conn.close()

class PerformanceAnalyzer:
    """æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self, db: PerformanceDatabase):
        self.db = db
    
    def generate_trend_report(self, crate_name: str, days: int = 30) -> Dict:
        """ç”Ÿæˆæ€§èƒ½è¶‹åŠ¿æŠ¥å‘Š"""
        conn = sqlite3.connect(self.db.db_path)
        
        # è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ•°æ®
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        
        df = pd.read_sql_query('''
            SELECT benchmark_name, duration_ns, timestamp
            FROM benchmark_results
            WHERE crate_name = ? AND timestamp BETWEEN ? AND ?
            ORDER BY timestamp
        ''', conn, params=(crate_name, start_date.isoformat(), end_date.isoformat()))
        
        conn.close()
        
        if df.empty:
            return {'error': f'æ²¡æœ‰æ‰¾åˆ° {crate_name} çš„æ•°æ®'}
        
        # è®¡ç®—è¶‹åŠ¿ç»Ÿè®¡
        report = {
            'crate_name': crate_name,
            'period_days': days,
            'total_benchmarks': len(df['benchmark_name'].unique()),
            'total_runs': len(df),
            'benchmarks': {}
        }
        
        for benchmark in df['benchmark_name'].unique():
            benchmark_data = df[df['benchmark_name'] == benchmark]
            durations = benchmark_data['duration_ns'].values
            
            # ç»Ÿè®¡ä¿¡æ¯
            stats_info = {
                'count': len(durations),
                'mean_ns': float(np.mean(durations)),
                'median_ns': float(np.median(durations)),
                'std_ns': float(np.std(durations)),
                'min_ns': int(np.min(durations)),
                'max_ns': int(np.max(durations)),
            }
            
            # è¶‹åŠ¿åˆ†æ
            if len(durations) >= 2:
                # çº¿æ€§å›å½’åˆ†æè¶‹åŠ¿
                x = np.arange(len(durations))
                slope, intercept, r_value, p_value, std_err = stats.linregress(x, durations)
                
                trend_info = {
                    'trend_slope': slope,
                    'trend_r_squared': r_value ** 2,
                    'trend_p_value': p_value,
                    'is_improving': slope < 0,  # æ—¶é—´å¢é•¿ä½†æ€§èƒ½æå‡ï¼ˆdurationå‡å°‘ï¼‰
                    'is_degrading': slope > 0 and p_value < 0.05  # ç»Ÿè®¡æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™
                }
                
                stats_info.update(trend_info)
            
            report['benchmarks'][benchmark] = stats_info
        
        return report
    
    def generate_comparison_report(self, base_commit: str, current_commit: str) -> Dict:
        """ç”Ÿæˆç‰ˆæœ¬å¯¹æ¯”æŠ¥å‘Š"""
        conn = sqlite3.connect(self.db.db_path)
        
        # è·å–ä¸¤ä¸ªç‰ˆæœ¬çš„æ•°æ®
        base_data = pd.read_sql_query('''
            SELECT crate_name, benchmark_name, AVG(duration_ns) as avg_duration
            FROM benchmark_results
            WHERE git_commit = ?
            GROUP BY crate_name, benchmark_name
        ''', conn, params=(base_commit,))
        
        current_data = pd.read_sql_query('''
            SELECT crate_name, benchmark_name, AVG(duration_ns) as avg_duration
            FROM benchmark_results
            WHERE git_commit = ?
            GROUP BY crate_name, benchmark_name
        ''', conn, params=(current_commit,))
        
        conn.close()
        
        # åˆå¹¶æ•°æ®è¿›è¡Œæ¯”è¾ƒ
        comparison = pd.merge(
            base_data,
            current_data,
            on=['crate_name', 'benchmark_name'],
            suffixes=('_base', '_current')
        )
        
        # è®¡ç®—å˜åŒ–
        comparison['change_ns'] = comparison['avg_duration_current'] - comparison['avg_duration_base']
        comparison['change_percent'] = (comparison['change_ns'] / comparison['avg_duration_base']) * 100
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'base_commit': base_commit,
            'current_commit': current_commit,
            'total_comparisons': len(comparison),
            'improvements': len(comparison[comparison['change_percent'] < -5]),  # æ€§èƒ½æå‡è¶…è¿‡5%
            'regressions': len(comparison[comparison['change_percent'] > 5]),    # æ€§èƒ½ä¸‹é™è¶…è¿‡5%
            'stable': len(comparison[comparison['change_percent'].abs() <= 5]),   # å˜åŒ–åœ¨5%ä»¥å†…
            'details': comparison.to_dict('records')
        }
        
        return report
    
    def export_visualization(self, report: Dict, output_path: str):
        """å¯¼å‡ºå¯è§†åŒ–å›¾è¡¨"""
        if 'benchmarks' not in report:
            logger.error("æŠ¥å‘Šæ ¼å¼é”™è¯¯ï¼Œæ— æ³•ç”Ÿæˆå›¾è¡¨")
            return
        
        crate_name = report['crate_name']
        benchmarks = report['benchmarks']
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(f'{crate_name} æ€§èƒ½åˆ†ææŠ¥å‘Š', fontsize=16, fontweight='bold')
        
        # 1. å¹³å‡æ‰§è¡Œæ—¶é—´æ¡å½¢å›¾
        benchmark_names = list(benchmarks.keys())
        mean_times = [benchmarks[name]['mean_ns'] / 1_000_000 for name in benchmark_names]  # è½¬æ¢ä¸ºæ¯«ç§’
        
        axes[0, 0].bar(range(len(benchmark_names)), mean_times, color='skyblue')
        axes[0, 0].set_title('å¹³å‡æ‰§è¡Œæ—¶é—´')
        axes[0, 0].set_ylabel('æ—¶é—´ (æ¯«ç§’)')
        axes[0, 0].set_xticks(range(len(benchmark_names)))
        axes[0, 0].set_xticklabels(benchmark_names, rotation=45, ha='right')
        
        # 2. å˜å¼‚ç³»æ•°ï¼ˆç¨³å®šæ€§æŒ‡æ ‡ï¼‰
        cv_values = []
        for name in benchmark_names:
            mean_val = benchmarks[name]['mean_ns']
            std_val = benchmarks[name]['std_ns']
            cv = (std_val / mean_val) * 100 if mean_val > 0 else 0
            cv_values.append(cv)
        
        axes[0, 1].bar(range(len(benchmark_names)), cv_values, color='lightcoral')
        axes[0, 1].set_title('å˜å¼‚ç³»æ•° (ç¨³å®šæ€§)')
        axes[0, 1].set_ylabel('å˜å¼‚ç³»æ•° (%)')
        axes[0, 1].set_xticks(range(len(benchmark_names)))
        axes[0, 1].set_xticklabels(benchmark_names, rotation=45, ha='right')
        
        # 3. è¶‹åŠ¿åˆ†æ
        trend_slopes = []
        for name in benchmark_names:
            if 'trend_slope' in benchmarks[name]:
                trend_slopes.append(benchmarks[name]['trend_slope'])
            else:
                trend_slopes.append(0)
        
        colors = ['green' if slope < 0 else 'red' for slope in trend_slopes]
        axes[1, 0].bar(range(len(benchmark_names)), trend_slopes, color=colors)
        axes[1, 0].set_title('æ€§èƒ½è¶‹åŠ¿ (è´Ÿå€¼è¡¨ç¤ºæ”¹å–„)')
        axes[1, 0].set_ylabel('è¶‹åŠ¿æ–œç‡')
        axes[1, 0].set_xticks(range(len(benchmark_names)))
        axes[1, 0].set_xticklabels(benchmark_names, rotation=45, ha='right')
        axes[1, 0].axhline(y=0, color='black', linestyle='--', alpha=0.5)
        
        # 4. æ€§èƒ½èŒƒå›´ï¼ˆæœ€å°å€¼-æœ€å¤§å€¼ï¼‰
        min_times = [benchmarks[name]['min_ns'] / 1_000_000 for name in benchmark_names]
        max_times = [benchmarks[name]['max_ns'] / 1_000_000 for name in benchmark_names]
        
        axes[1, 1].fill_between(range(len(benchmark_names)), min_times, max_times, alpha=0.3, color='purple')
        axes[1, 1].plot(range(len(benchmark_names)), mean_times, color='purple', marker='o')
        axes[1, 1].set_title('æ€§èƒ½èŒƒå›´ (æœ€å°å€¼-æœ€å¤§å€¼)')
        axes[1, 1].set_ylabel('æ—¶é—´ (æ¯«ç§’)')
        axes[1, 1].set_xticks(range(len(benchmark_names)))
        axes[1, 1].set_xticklabels(benchmark_names, rotation=45, ha='right')
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {output_path}")

def load_benchmark_results(file_path: str) -> List[BenchmarkResult]:
    """ä»JSONæ–‡ä»¶åŠ è½½åŸºå‡†æµ‹è¯•ç»“æœ"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    results = []
    if isinstance(data, list):
        for item in data:
            result = BenchmarkResult(
                crate_name=item['crate_name'],
                benchmark_name=item['benchmark_name'],
                duration_ns=item['duration_ns'],
                memory_usage_bytes=item.get('memory_usage_bytes', 0),
                cpu_utilization_percent=item.get('cpu_utilization_percent', 0.0),
                timestamp=item['timestamp'],
                git_commit=item.get('git_commit'),
                metadata=item.get('metadata')
            )
            results.append(result)
    
    return results

def main():
    parser = argparse.ArgumentParser(description='ModuForge-RS æ€§èƒ½æŒ‡æ ‡ç®¡ç†å·¥å…·')
    parser.add_argument('--db', default='benchmarks/performance.db', help='æ•°æ®åº“è·¯å¾„')
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # å¯¼å…¥å‘½ä»¤
    import_parser = subparsers.add_parser('import', help='å¯¼å…¥åŸºå‡†æµ‹è¯•ç»“æœ')
    import_parser.add_argument('file', help='ç»“æœæ–‡ä»¶è·¯å¾„ (JSONæ ¼å¼)')
    
    # åŸºçº¿å‘½ä»¤
    baseline_parser = subparsers.add_parser('baseline', help='è®¾ç½®æ€§èƒ½åŸºçº¿')
    baseline_parser.add_argument('--crate', required=True, help='Crateåç§°')
    baseline_parser.add_argument('--benchmark', required=True, help='åŸºå‡†æµ‹è¯•åç§°')
    baseline_parser.add_argument('--duration', type=int, required=True, help='åŸºçº¿æ—¶é—´ (çº³ç§’)')
    baseline_parser.add_argument('--commit', required=True, help='Gitæäº¤å“ˆå¸Œ')
    
    # æ£€æµ‹å‘½ä»¤
    detect_parser = subparsers.add_parser('detect', help='æ£€æµ‹æ€§èƒ½å›å½’')
    detect_parser.add_argument('file', help='å½“å‰ç»“æœæ–‡ä»¶è·¯å¾„')
    detect_parser.add_argument('--threshold', type=float, default=10.0, help='å›å½’é˜ˆå€¼ç™¾åˆ†æ¯”')
    
    # æŠ¥å‘Šå‘½ä»¤
    report_parser = subparsers.add_parser('report', help='ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š')
    report_parser.add_argument('--crate', required=True, help='Crateåç§°')
    report_parser.add_argument('--days', type=int, default=30, help='åˆ†æå¤©æ•°')
    report_parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    report_parser.add_argument('--format', choices=['json', 'chart'], default='json', help='è¾“å‡ºæ ¼å¼')
    
    # æ¯”è¾ƒå‘½ä»¤
    compare_parser = subparsers.add_parser('compare', help='æ¯”è¾ƒä¸¤ä¸ªç‰ˆæœ¬çš„æ€§èƒ½')
    compare_parser.add_argument('--base', required=True, help='åŸºå‡†ç‰ˆæœ¬æäº¤å“ˆå¸Œ')
    compare_parser.add_argument('--current', required=True, help='å½“å‰ç‰ˆæœ¬æäº¤å“ˆå¸Œ')
    compare_parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # åˆå§‹åŒ–æ•°æ®åº“
    db = PerformanceDatabase(args.db)
    
    if args.command == 'import':
        results = load_benchmark_results(args.file)
        db.insert_results(results)
        print(f"âœ… æˆåŠŸå¯¼å…¥ {len(results)} æ¡ç»“æœ")
    
    elif args.command == 'baseline':
        db.set_baseline(args.crate, args.benchmark, args.duration, args.commit)
        print(f"âœ… åŸºçº¿è®¾ç½®å®Œæˆ: {args.crate}/{args.benchmark}")
    
    elif args.command == 'detect':
        results = load_benchmark_results(args.file)
        detector = RegressionDetector(db, args.threshold)
        alerts = detector.detect_regressions(results)
        
        if alerts:
            print(f"ğŸš¨ æ£€æµ‹åˆ° {len(alerts)} ä¸ªæ€§èƒ½å›å½’:")
            for alert in alerts:
                print(f"  - {alert['crate_name']}/{alert['benchmark_name']}: "
                      f"{alert['regression_percent']:+.1f}% ({alert['severity']})")
        else:
            print("âœ… æœªæ£€æµ‹åˆ°æ€§èƒ½å›å½’")
    
    elif args.command == 'report':
        analyzer = PerformanceAnalyzer(db)
        report = analyzer.generate_trend_report(args.crate, args.days)
        
        if args.format == 'json':
            output_file = args.output or f'{args.crate}_report.json'
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            print(f"ğŸ“Š æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        elif args.format == 'chart':
            output_file = args.output or f'{args.crate}_chart.png'
            analyzer.export_visualization(report, output_file)
    
    elif args.command == 'compare':
        analyzer = PerformanceAnalyzer(db)
        report = analyzer.generate_comparison_report(args.base, args.current)
        
        output_file = args.output or f'comparison_{args.base[:8]}_vs_{args.current[:8]}.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        print(f"   æ”¹å–„: {report['improvements']}")
        print(f"   å›å½’: {report['regressions']}")
        print(f"   ç¨³å®š: {report['stable']}")

if __name__ == '__main__':
    main()