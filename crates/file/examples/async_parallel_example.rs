use mf_file::*;
use std::time::Instant;
use std::path::PathBuf;
use serde_json;

#[tokio::main]
async fn main() -> Result<()> {
    use mf_file::{
        async_document::{AsyncDocumentWriter, AsyncDocumentReader},
        parallel_compression::ParallelCompressionConfig,
        document::SegmentType,
    };

    println!("=== å¼‚æ­¥æ–‡æ¡£è¯»å†™æ¼”ç¤º ===\n");

    // ä½¿ç”¨çœŸå®ç›®å½• - åœ¨å½“å‰ç›®å½•ä¸‹åˆ›å»º test_data æ–‡ä»¶å¤¹
    let dir = PathBuf::from("test_data");
    tokio::fs::create_dir_all(&dir).await.ok();

    let path = dir.join("async_doc.mff");
    let start = Instant::now();

    // å‡†å¤‡æµ‹è¯•æ•°æ®
    let test_metadata = serde_json::json!({
        "version": "1.0.0",
        "author": "å¼‚æ­¥ç¤ºä¾‹",
        "created": "2024-01-01T00:00:00Z",
        "description": "å¸¦å¹¶è¡Œå‹ç¼©çš„æµ‹è¯•æ–‡æ¡£"
    });

    let test_config = serde_json::json!({
        "compression": "å¹¶è¡Œ",
        "chunk_size": 512 * 1024,
        "level": 3
    });

    // ç”ŸæˆçœŸå®æ•°æ®
    let mut large_json_data = Vec::new();
    for i in 0..10000 {
        let record = serde_json::json!({
            "id": i,
            "åç§°": format!("è®°å½• {}", i),
            "æ•°å€¼": i as f64 * 3.14159,
            "æ—¶é—´æˆ³": 1704067200 + i,
            "æ ‡ç­¾": vec![format!("æ ‡ç­¾{}", i % 10), format!("åˆ†ç±»{}", i % 5)]
        });
        large_json_data.push(record);
    }
    let large_json_bytes =
        serde_json::to_vec(&large_json_data).map_err(|e| {
            FileError::Io(std::io::Error::new(std::io::ErrorKind::Other, e))
        })?;

    // ç”Ÿæˆå¸¦æ¨¡å¼çš„äºŒè¿›åˆ¶æ•°æ®
    let mut binary_data = Vec::with_capacity(2 * 1024 * 1024);
    for i in 0..2 * 1024 * 1024 {
        binary_data.push(((i * 7 + 13) % 256) as u8);
    }

    println!("å†™å…¥æ–‡æ¡£åˆ°: {}", path.display());

    // ä½¿ç”¨å¹¶è¡Œå‹ç¼©å†™å…¥æ–‡æ¡£
    let bytes_written = {
        let compression_config = ParallelCompressionConfig {
            level: 3,
            chunk_size: 512 * 1024,
            parallel_threshold: 100 * 1024, // 100KB é˜ˆå€¼
            ..Default::default()
        };

        let writer = AsyncDocumentWriter::begin_with_config(
            &path,
            compression_config,
            true, // å¯ç”¨å¹¶è¡Œå‹ç¼©
        )
        .await?;

        // æ·»åŠ å„ç§ç±»å‹çš„æ®µ
        let segments = vec![
            (
                SegmentType("å…ƒæ•°æ®".to_string()),
                serde_json::to_vec(&test_metadata).map_err(|e| {
                    FileError::Io(std::io::Error::new(
                        std::io::ErrorKind::Other,
                        e,
                    ))
                })?,
            ),
            (
                SegmentType("é…ç½®".to_string()),
                serde_json::to_vec(&test_config).map_err(|e| {
                    FileError::Io(std::io::Error::new(
                        std::io::ErrorKind::Other,
                        e,
                    ))
                })?,
            ),
            (SegmentType("JSONæ•°æ®".to_string()), large_json_bytes.clone()),
            (SegmentType("äºŒè¿›åˆ¶æ•°æ®".to_string()), binary_data.clone()),
            (
                SegmentType("æ–‡æœ¬æ•°æ®".to_string()),
                "è¿™æ˜¯ä¸€ä¸ªå°†è¢«å‹ç¼©çš„ç¤ºä¾‹æ–‡æœ¬æ®µã€‚"
                    .repeat(1000)
                    .as_bytes()
                    .to_vec(),
            ),
        ];

        let total_size: usize =
            segments.iter().map(|(_, data)| data.len()).sum();
        println!(
            "- æœªå‹ç¼©æ€»å¤§å°: {:.2} MB",
            total_size as f64 / 1024.0 / 1024.0
        );

        // æ‰¹é‡æ·»åŠ ï¼ˆå¹¶è¡Œï¼‰
        writer.add_segments_batch(segments).await?;
        writer.finalize().await?;

        total_size
    };

    // è·å–å‹ç¼©åçš„æ–‡ä»¶å¤§å°
    let file_metadata = tokio::fs::metadata(&path).await?;
    let compressed_size = file_metadata.len();
    println!(
        "- å‹ç¼©åæ–‡ä»¶å¤§å°: {:.2} MB",
        compressed_size as f64 / 1024.0 / 1024.0
    );
    println!(
        "- å‹ç¼©ç‡: {:.1}%",
        compressed_size as f64 / bytes_written as f64 * 100.0
    );

    // è¯»å–æ–‡æ¡£
    println!("\næ­£åœ¨è¯»å–æ–‡æ¡£...");
    {
        let reader = AsyncDocumentReader::open(&path).await?;

        // è·å–æ–‡æ¡£ä¿¡æ¯
        let dir = reader.directory();
        println!("- æ–‡æ¡£åŒ…å« {} ä¸ªæ®µ", dir.entries.len());
        println!("- å‹ç¼©æ ‡å¿—: 0x{:04X}", dir.flags);

        // è·å–å¹¶éªŒè¯å…ƒæ•°æ®
        let metadata_bytes = reader
            .get_segment(&SegmentType("å…ƒæ•°æ®".to_string()))
            .await?
            .unwrap();
        let read_metadata: serde_json::Value =
            serde_json::from_slice(&metadata_bytes).map_err(|e| {
                FileError::Io(std::io::Error::new(std::io::ErrorKind::Other, e))
            })?;
        println!("- å…ƒæ•°æ®éªŒè¯é€šè¿‡: ç‰ˆæœ¬={}", read_metadata["version"]);

        // è·å–å¹¶éªŒè¯é…ç½®
        let config_bytes = reader
            .get_segment(&SegmentType("é…ç½®".to_string()))
            .await?
            .unwrap();
        let read_config: serde_json::Value =
            serde_json::from_slice(&config_bytes).map_err(|e| {
                FileError::Io(std::io::Error::new(std::io::ErrorKind::Other, e))
            })?;
        println!("- é…ç½®éªŒè¯é€šè¿‡: å‹ç¼©={}", read_config["compression"]);

        // éªŒè¯ JSON æ•°æ®
        let json_bytes = reader
            .get_segment(&SegmentType("JSONæ•°æ®".to_string()))
            .await?
            .unwrap();
        let read_json_data: Vec<serde_json::Value> =
            serde_json::from_slice(&json_bytes).map_err(|e| {
                FileError::Io(std::io::Error::new(std::io::ErrorKind::Other, e))
            })?;
        println!("- JSON æ•°æ®éªŒè¯é€šè¿‡: {} æ¡è®°å½•", read_json_data.len());

        // éªŒè¯äºŒè¿›åˆ¶æ•°æ®
        let binary_bytes = reader
            .get_segment(&SegmentType("äºŒè¿›åˆ¶æ•°æ®".to_string()))
            .await?
            .unwrap();
        assert_eq!(binary_bytes, binary_data);
        println!("- äºŒè¿›åˆ¶æ•°æ®éªŒè¯é€šè¿‡: {} å­—èŠ‚", binary_bytes.len());

        // æµå¼è¯»å–æ‰€æœ‰æ®µå¹¶è®¡ç®—å¤§å°
        use futures::StreamExt;
        let stream = reader.stream_segments();
        futures::pin_mut!(stream);
        let mut total_decompressed = 0;
        let mut segment_info = Vec::new();
        while let Some(result) = stream.next().await {
            let (kind, data) = result?;
            total_decompressed += data.len();
            segment_info.push((kind.0.clone(), data.len()));
        }
        println!("\nè§£å‹åçš„æ®µå¤§å°:");
        for (name, size) in &segment_info {
            println!("  - {}: {:.2} KB", name, *size as f64 / 1024.0);
        }
        println!(
            "- è§£å‹åæ€»å¤§å°: {:.2} MB",
            total_decompressed as f64 / 1024.0 / 1024.0
        );

        // å¹¶è¡Œå¤„ç†æ‰€æœ‰æ®µè¿›è¡Œåˆ†æ
        let analysis_start = Instant::now();
        let results = reader
            .process_all_parallel(|kind, data| {
                // æ¨¡æ‹Ÿæ•°æ®å¤„ç†
                let checksum = data
                    .iter()
                    .fold(0u64, |acc, &b| acc.wrapping_add(b as u64));
                (kind.0, data.len(), checksum)
            })
            .await?;
        println!("\nå¹¶è¡Œåˆ†æå®Œæˆï¼Œè€—æ—¶ {:?}", analysis_start.elapsed());
        println!("åˆ†æç»“æœ:");
        for (name, size, checksum) in &results {
            println!("  - {}: å¤§å°={}, æ ¡éªŒå’Œ={}", name, size, checksum);
        }

        // è·å–æ–‡æ¡£é•¿åº¦
        let doc_len = reader.logical_len().await;
        println!("\næ–‡æ¡£é€»è¾‘é•¿åº¦: {} å­—èŠ‚", doc_len);
    }

    println!("\næ¼”ç¤ºæ€»è€—æ—¶: {:?}", start.elapsed());

    // æ˜¾ç¤ºæœ€ç»ˆæ–‡ä»¶ä¿¡æ¯
    println!("\næœ€ç»ˆæ–‡ä»¶ä¿¡æ¯:");
    println!("- è·¯å¾„: {}", path.display());
    println!("- å¤§å°: {} å­—èŠ‚", compressed_size);
    println!("- æˆåŠŸå†™å…¥å¹¶éªŒè¯ âœ“");

    println!("\nâœ… æ¼”ç¤ºæˆåŠŸå®Œæˆï¼");
    println!("ğŸ“ æ–‡ä»¶ä¿å­˜åœ¨: {}", path.display());

    Ok(())
}
